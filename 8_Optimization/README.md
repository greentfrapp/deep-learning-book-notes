# 8 Optimization for Training Deep Models

`page 268`

> We therefore optimize P only indirectly.

The optimization is indirect in two ways.

First, we optimize over the training (empirical) distribution, although we are actually concerned about the data-generating distribution. We typically take the training distribution as being approximately representative of the underlying data-generating distribution. Differences in performance then appears in the form of generalization error, which we try to mitigate with regularization measures.

Second, the quantity being optimized is frequently different from the quantity we are actually concerned about. This is usually because the latter is non-differentiable. For example, when training a classifier, we typically minimize the cross-entropy loss between the prediction and the label. But what we actually want is to maximize the accuracy. In particular, although the cross-entropy loss and accuracy are mostly aligned, they still have a non-monotonic relationship ie. an decrease in cross-entropy loss does not necessarily imply an increase (or decrease) in accuracy.

`page 272`

> Small batches can offer a regularizing effect

And on that note, large batches can give the opposite effect and lead to faster overfitting. As mentioned in the text, it is important to increase the nubmer of steps when using smaller batches, since it will take more steps to finish a single epoch (one run through the entire training set). Also mentioned, a smaller learning rate might also be needed due to the increase in noise for smaller batch sizes.

`page 273`

> Failing to ever shuffle the examples in any way can seriously reduce the effectiveness of the algorithm.

A similar problem was encountered in reinforcement learning, where the experiences used to the agent is highly correlated and chronological. A large improvement was achieved with *experience replay* - storing and randomly sampling from a set of experiences for learning - which has become a popular technique used in reinforcement learning.

`page 276` Ideally, when we arrive at the global or a local minimum of the loss function, the local gradient should be zero (actually it should be zero at any stationary point). However, Figure 8.1 shows that this does not actually happen in practice. Instead, the figure shows the gradient norm actually increasing. Yet, we see that the corresponding validation classification error does decrease to a low level.

`page 277` **Weight Space Symmetry.** This is also somewhat related to the problem of few-shot classification, where a typical task might be 5-way 1-shot (ie. 5 classes with 1 example each). Depending on the algorithm and approach, rearranging the classes for a certain task can be interpreted as a new task (eg. switching classes 1 and 2). The metalearner has to be invariant to these changes and adaptive measures have to be taken to allow the metalearner to be 'symmetrical'.

`page 277`

> Whether newtons of practical interest home many local minima of high cost [...]

I believe there is a typo here, where *newtons* should be corrected to *networks*. This errata is still present in the [web version](http://www.deeplearningbook.org/contents/optimization.html) (see page 282). 

`page 278`

> To understand the intuition behind this behavior, observe that the Hessian matrix at a local minimum has only positive eigenvalues. The Hessian matrix at a saddle point has a mixture of positive and negative values. Imagine that the sign of each eigenvalue is generated by flipping a coin.

A pretty good explanation of why saddle points are far more common in high-dimensional spaces, although I'm not sure if the coin-flipping analogy holds in a mathematical sense.

`page 281`

> The cliff can be dangerous whether we approach it from above or from below [...]

Approaching it from above, the huge gradient due to the cliff leads to the gradient descent algorithm taking an excessibly large step, which might 'reset' and learning done by the weights.

Approaching it from below, a small step taken towards the cliff (perhaps due to a downward slope just before the cliff) will cause a sudden sharp increase in the loss as we jump to the top of the cliff.

`page 281` **Gradient Clipping.** The text mentions it as a solution against excessively large gradient steps due to cliffs. Another way to think about it is to see that gradient clipping prevents overflows or underflows causing exploding or vanishing gradients.

`page 286`

> By comparison, the true gradient of the total cost function becomes small and then 0 when we approach and reach a minimum using batch gradient descent, so batch gradient descent can use a fixed learning rate.

Note that this refers to **batch** gradient descent and not **minibatch** gradient descent. So we are calculating a single gradient step based on the entire training set.

`page 287`

The conditions on epsilon for convergence of SGD is similar to the conditions on epsilon for the epsilon-greedy policy in reinforcement learning, such that the policy is Greedy in the Limit with Infinite Exploration (GLIE) and leads to convergence to the optimal policy.

`page 288` **Momentum.** A nice Distill [article](https://distill.pub/2017/momentum/) on momentum (Goh, 2017).

The ![\alpha](http://latex.codecogs.com/gif.latex?%5Calpha) parameter is essentially how much of the previous gradient you want to keep, in order to compute the new gradient.

`page 290` Equation 8.17 can be easily derived from ![\mathbf{v}^{(i)}=\alpha\mathbf{v}^{(i-1)}-\epsilon\mathbf{g}](http://latex.codecogs.com/gif.latex?%5Cmathbf%7Bv%7D%5E%7B%28i%29%7D%3D%5Calpha%5Cmathbf%7Bv%7D%5E%7B%28i-1%29%7D-%5Cepsilon%5Cmathbf%7Bg%7D) by setting ![\mathbf{v}^{(i)}=\mathbf{v}^{(i-1)}](http://latex.codecogs.com/gif.latex?%5Cmathbf%7Bv%7D%5E%7B%28i%29%7D%3D%5Cmathbf%7Bv%7D%5E%7B%28i-1%29%7D) ie. the definition of terminal velocity.

`page 291` **Nesterov Momentum.** Notice in Equation 8.21 that we use ![\mathbf{\theta}+\alpha\mathbf{v}](http://latex.codecogs.com/gif.latex?%5Cmathbf%7B%5Ctheta%7D&plus;%5Calpha%5Cmathbf%7Bv%7D) for calculating the loss and the loss gradient, instead of ![\mathbf{\theta}](http://latex.codecogs.com/gif.latex?%5Cmathbf%7B%5Ctheta%7D) as in Equation 8.15.

This essentially means at every step, we calculate the new gradient by saying, 'Okay, what if we first apply the previous gradient step again (multiplied by alpha)'. So, the new gradient takes into account the momentum from the previous gradient step, ie. the *correction factor* mentioned in the text.

`page 293` Gram-Schmidt orthogonalization ensures that the weight matrix is orthonormal ie. the vectors are linearly independent (orthogonal) and normalized to unit norm.

`page 294` Second paragraph has very interesting notes on how SGD "expresses a prior that the final parameters should be close to the initial parameters", which lends meaning to how we initialize the weight parameters.

`page 295` In Equation 8.23, the normalizing over ![m+n](http://latex.codecogs.com/gif.latex?m&plus;n) can be attributed to how the magnitude of the dot-product output is affected by the dimension of the weight matrix. A similar concern is expressed by Vaswani et al. ([2017](https://arxiv.org/abs/1706.03762)) when discussing dot-product attention, where the authors also use a similar normalization method.

A drawback is that the normalization results in very small values when the layers are large. This is mentioned in the following page.

`page 299` **RMSProp.** On an interesting note, RMSProp was actually introduced in Geoffrey Hinton's Coursera videos and never formally published in a paper. The citation for this section also leads to the Coursera lectures.

`page 310` Great note on the importance of backpropagating through the batch-norm operations.

`page 312` Actually, despite the recommendations by Ioffe and Szegedy ([2015](https://arxiv.org/abs/1502.03167)), there appears to be significant debate over whether to perform batchnorm before or after the activation (see [here](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/)).

`page 312`

> In convolutional networks, described in chapter 9, it is important to apply the same normalizing ![\mu] and ![\sigma] at every spatial location within a feature map, so that the statistics of the feature map remain the same regardless of spatial location.

For convolutional networks, we apply the batchnorm to the channels axis (see note on axis argument in Tensorflow's batchnorm [documentation](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization)).

`page 315` The concept behind FitNets ([Romero et al., 2015](https://arxiv.org/abs/1412.6550)) and the other pretraining ideas here seem to be related to the concept of distillation ([Hinton et al., 2015](https://arxiv.org/abs/1503.02531)).

`page 317` Skip connections (aka residual connections) also feature prominently in ResNets ([He et al., 2015](https://arxiv.org/abs/1512.03385))

`page 320` Interesting note on how a stochastic curriculum might give better results than a deterministic curriculum. On a side note, curriculum learning is also related to Saliman & Chen's ([2018](https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/)) work on reinforcement learning from a single human demonstration, which achieved SOTA results on the notoriously difficult (for AI) game Montezuma's Revenge. The main idea involves having a single human demonstration, rewinding the replay by a few steps and then training the agent to complete the demonstration. For example, we might train the initial agent to start from the last n steps of the human replay, where n=1. We then iteratively increase n when the agent is able to achieve a result that matches or surpasses the human's performance, until the agent is able to play the entire game from the beginning.
